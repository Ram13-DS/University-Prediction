# University-Prediction

This project aims to use machine learning techniques to help students select a university. The project will collect data on studentâ€™s interests, and goals, and use this data to train a machine learning model. The model will then be used to predict which universities are the best fit for each student. Choosing a university is a major decision for many students. There are many factors to consider, such as academic reputation, cost, location, and extracurricular activities. In recent years, machine learning has been used to help students make more informed decisions about university selection

**Algorithms used**

1.LOGISTIC REGRESSION

A categorical dependent variable (also known as the outcome variable) and one or more independent variables (also known as predictor or explanatory variables) are analyzed using the statistical technique of logistic regression. Binary outcomes, such as yes/no, true/false, or 1/0, are modelled using this particular sort of regression analysis.

2.K NEAREST NEIGHBOR

A supervised learning technique used for classification and regression analysis is the K-Nearest Neighbor (KNN) classifier. KNN is a non-parametric method that makes no assumptions about how the data are distributed in their underlying form.

3.RANDOM FOREST CLASSIFIER 

In Random Forest, a number of decision trees are trained using a random selection of features at each split on various subsets of the training data. This produces a varied collection of decision trees that can effectively capture various features of the data. Each decision tree in the forest separately forecasts the input's class label during the prediction phase, and the final prediction is based on the trees' majority vote.

4.EXTRA TREES  CLASSIFIER

Extra Trees Classifier constructs a number of decision trees and then combines their outputs to make predictions, similar to other ensemble approaches. In contrast to other decision tree algorithms, it randomly chooses feature subsets for each decision tree and divides nodes. This randomization aids in lowering the model's variance and guards against overfitting.
